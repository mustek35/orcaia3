# patch_input_size_fix.py
"""
Segundo patch para solucionar el error 'input_size' en CUDA Processor
"""
import os
import shutil
from datetime import datetime

def apply_input_size_patch():
    """Aplicar patch para el error 'input_size'"""
    
    processor_file = "core/cuda_pipeline_processor.py"
    
    if not os.path.exists(processor_file):
        print(f"âŒ Archivo {processor_file} no encontrado")
        return False
    
    print(f"ğŸ”§ Aplicando patch para 'input_size'...")
    
    try:
        # Leer archivo
        with open(processor_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Crear backup
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = f"{processor_file}.backup_inputsize_{timestamp}"
        shutil.copy2(processor_file, backup_file)
        print(f"ğŸ’¾ Backup creado: {backup_file}")
        
        # Patch 1: Fix en __init__ para aÃ±adir input_size
        init_patch_original = """def __init__(self, processor_id, model_config, device='cuda:0'):
        # Aplicar configuraciones por defecto para evitar KeyErrors
        default_config = {
            'compile_model': False,
            'optimize_for_inference': True,
            'warmup_iterations': 3,
            'half_precision': True,
            'verbose': False
        }
        self.model_config = {**default_config, **model_config}"""
        
        init_patch_new = """def __init__(self, processor_id, model_config, device='cuda:0'):
        # Aplicar configuraciones por defecto para evitar KeyErrors
        default_config = {
            'compile_model': False,
            'optimize_for_inference': True,
            'warmup_iterations': 3,
            'half_precision': True,
            'verbose': False,
            'input_size': 640,  # TamaÃ±o de entrada por defecto
            'imgsz': 640,       # Alias para input_size
            'max_det': 300,     # Detecciones mÃ¡ximas
            'iou_threshold': 0.45,  # Umbral IoU
            'classes': None,    # Todas las clases
            'agnostic_nms': False,  # NMS agnÃ³stico de clase
        }
        self.model_config = {**default_config, **model_config}"""
        
        if init_patch_original in content:
            content = content.replace(init_patch_original, init_patch_new)
            print("âœ… Patch 1: ConfiguraciÃ³n por defecto expandida")
        
        # Patch 2: Fix acceso directo a input_size
        patches_direct_access = [
            {
                'find': "self.model_config['input_size']",
                'replace': "self.model_config.get('input_size', 640)",
                'desc': "Fix acceso directo input_size"
            },
            {
                'find': "self.model_config['imgsz']", 
                'replace': "self.model_config.get('imgsz', self.model_config.get('input_size', 640))",
                'desc': "Fix acceso directo imgsz"
            },
            {
                'find': "self.model_config['max_det']",
                'replace': "self.model_config.get('max_det', 300)",
                'desc': "Fix acceso directo max_det"
            },
            {
                'find': "self.model_config['iou_threshold']",
                'replace': "self.model_config.get('iou_threshold', 0.45)",
                'desc': "Fix acceso directo iou_threshold"
            },
            {
                'find': "self.model_config['classes']",
                'replace': "self.model_config.get('classes', None)",
                'desc': "Fix acceso directo classes"
            },
            {
                'find': "self.model_config['agnostic_nms']",
                'replace': "self.model_config.get('agnostic_nms', False)",
                'desc': "Fix acceso directo agnostic_nms"
            }
        ]
        
        patches_applied = 0
        for patch in patches_direct_access:
            if patch['find'] in content:
                content = content.replace(patch['find'], patch['replace'])
                patches_applied += 1
                print(f"âœ… {patch['desc']}")
        
        # Patch 3: Fix en preprocessing especÃ­fico
        preprocessing_patch_find = """def _preprocess_frame(self, frame):
        """
        
        preprocessing_patch_replace = """def _preprocess_frame(self, frame):
        try:
            # Obtener tamaÃ±o de entrada de forma segura
            input_size = self.model_config.get('input_size', 
                        self.model_config.get('imgsz', 640))
            
            if isinstance(input_size, (list, tuple)):
                target_size = input_size[:2]  # (height, width)
            else:
                target_size = (input_size, input_size)  # cuadrado
        """
        
        if preprocessing_patch_find in content and preprocessing_patch_replace not in content:
            content = content.replace(preprocessing_patch_find, preprocessing_patch_replace)
            print("âœ… Patch preprocessing function")
            patches_applied += 1
        
        # Escribir archivo modificado
        if patches_applied > 0:
            with open(processor_file, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"âœ… Segundo patch aplicado ({patches_applied} modificaciones)")
            return True
        else:
            print("âš ï¸ No se encontraron patrones para patchear")
            return True
            
    except Exception as e:
        print(f"âŒ Error aplicando patch: {e}")
        return False

def create_complete_test():
    """Crear test final completo"""
    
    test_content = '''# test_final_complete.py
"""
Test final despuÃ©s de todos los patches
"""
import time
import numpy as np

def test_complete_pipeline():
    print("ğŸ§ª TEST PIPELINE COMPLETO")
    print("=" * 50)
    
    try:
        from core.cuda_pipeline_processor import cuda_pipeline_manager
        
        # ConfiguraciÃ³n ULTRA COMPLETA
        config = {
            'model_path': 'yolov8n.pt',
            'batch_size': 1,
            'confidence_threshold': 0.5,
            'half_precision': True,
            'compile_model': False,
            'optimize_for_inference': True,
            'warmup_iterations': 1,
            'max_det': 100,
            'iou_threshold': 0.45,
            'device': 'cuda:0',
            'verbose': False,
            'input_size': 640,      # âœ… CLAVE AÃ‘ADIDA
            'imgsz': 640,          # âœ… CLAVE AÃ‘ADIDA
            'classes': None,        # âœ… CLAVE AÃ‘ADIDA
            'agnostic_nms': False, # âœ… CLAVE AÃ‘ADIDA
            'memory_fraction': 0.8,
            'allow_growth': True,
            'num_workers': 1,
            'prefetch_factor': 2,
            'pin_memory': True,
            'processing_timeout': 5.0,
            'max_queue_size': 10,
            'clear_cache_interval': 100
        }
        
        print("ğŸ“¦ Creando procesador...")
        processor = cuda_pipeline_manager.create_processor('final_test', config)
        
        print("ğŸ”§ Cargando modelo...")
        processor.load_model()
        
        print("ğŸš€ Iniciando procesamiento...")
        processor.start_processing()
        
        # Test con frame realista
        test_frame = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
        
        results_count = 0
        results_received = []
        
        def on_results(detections, metadata):
            nonlocal results_count, results_received
            results_count += 1
            results_received.append((len(detections), metadata))
            print(f"ğŸ¯ Resultado {results_count}: {len(detections)} detecciones")
        
        processor.results_ready.connect(on_results)
        
        # Enviar frames
        print("ğŸ“¤ Enviando frames...")
        for i in range(5):
            metadata = {
                'frame_id': i,
                'timestamp': time.time(),
                'source': 'test_final'
            }
            processor.add_frame(test_frame, metadata)
            time.sleep(0.2)
        
        # Esperar resultados
        print("â³ Esperando resultados...")
        time.sleep(3)
        
        # Cleanup
        processor.stop_processing()
        cuda_pipeline_manager.stop_all_processors()
        
        # Resultados
        print("\\nğŸ“Š RESULTADOS:")
        print(f"   ğŸ“¦ Frames enviados: 5")
        print(f"   ğŸ¯ Resultados recibidos: {results_count}")
        print(f"   âœ… Pipeline funcional: {'SÃ­' if results_count > 0 else 'No'}")
        
        if results_received:
            for i, (det_count, meta) in enumerate(results_received):
                print(f"   ğŸ“‹ Frame {i}: {det_count} detecciones")
        
        return results_count > 0
        
    except Exception as e:
        print(f"âŒ Error en pipeline: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_rtsp_integration():
    """Test integraciÃ³n RTSP + CUDA"""
    print("\\nğŸ¥ TEST INTEGRACIÃ“N RTSP + CUDA")
    print("=" * 50)
    
    try:
        import cv2
        from core.cuda_pipeline_processor import cuda_pipeline_manager
        
        # Configurar RTSP
        rtsp_url = "rtsp://root:%40Remoto753524@19.10.10.132:554/media/video1"
        
        cap = cv2.VideoCapture()
        cap.set(cv2.CAP_PROP_OPEN_TIMEOUT_MSEC, 10000)
        cap.set(cv2.CAP_PROP_READ_TIMEOUT_MSEC, 5000)
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        if not cap.open(rtsp_url, cv2.CAP_FFMPEG):
            print("âŒ No se pudo conectar a RTSP")
            return False
        
        print("âœ… RTSP conectado")
        
        # Configurar procesador
        config = {
            'model_path': 'yolov8n.pt',
            'batch_size': 1,
            'confidence_threshold': 0.3,
            'input_size': 640,
            'compile_model': False,
            'half_precision': True,
            'optimize_for_inference': True,
            'warmup_iterations': 1
        }
        
        processor = cuda_pipeline_manager.create_processor('rtsp_test', config)
        processor.load_model()
        processor.start_processing()
        
        results_count = 0
        
        def on_results(detections, metadata):
            nonlocal results_count
            results_count += 1
            print(f"ğŸ¯ Frame {metadata.get('frame_id', '?')}: {len(detections)} detecciones")
        
        processor.results_ready.connect(on_results)
        
        # Procesar frames reales
        print("ğŸ“¹ Procesando frames RTSP...")
        for i in range(10):
            ret, frame = cap.read()
            if ret:
                processor.add_frame(frame, {'frame_id': i, 'source': 'rtsp'})
                time.sleep(0.1)
            else:
                print(f"âš ï¸ Fallo leyendo frame {i}")
                break
        
        time.sleep(2)  # Esperar procesamiento
        
        # Cleanup
        cap.release()
        processor.stop_processing()
        cuda_pipeline_manager.stop_all_processors()
        
        print(f"\\nâœ… IntegraciÃ³n RTSP+CUDA: {results_count} frames procesados")
        return results_count > 0
        
    except Exception as e:
        print(f"âŒ Error integraciÃ³n: {e}")
        return False

def main():
    print("ğŸ TEST FINAL COMPLETO")
    print("=" * 60)
    
    # Test 1: Pipeline puro
    pipeline_ok = test_complete_pipeline()
    
    # Test 2: IntegraciÃ³n RTSP
    integration_ok = test_rtsp_integration()
    
    print("\\n" + "=" * 60)
    print("ğŸ¯ RESULTADO FINAL:")
    print(f"   âš™ï¸  Pipeline CUDA:      {'âœ…' if pipeline_ok else 'âŒ'}")
    print(f"   ğŸ¥ IntegraciÃ³n RTSP:   {'âœ…' if integration_ok else 'âŒ'}")
    
    if pipeline_ok and integration_ok:
        print("\\nğŸ‰ Â¡SISTEMA COMPLETAMENTE FUNCIONAL!")
        print("ğŸš€ Puedes proceder con el desarrollo del proyecto")
    elif pipeline_ok:
        print("\\nâš ï¸ Pipeline funciona, pero hay problemas con RTSP")
        print("ğŸ’¡ Desarrolla primero con datos sintÃ©ticos")
    else:
        print("\\nâŒ AÃºn hay problemas con el pipeline CUDA")
        print("ğŸ’¡ Revisa los logs de error arriba")

if __name__ == "__main__":
    main()
'''
    
    with open('test_final_complete.py', 'w', encoding='utf-8') as f:
        f.write(test_content)
    
    print("âœ… test_final_complete.py creado")

def main():
    print("ğŸ”§ SEGUNDO PATCH - FIX INPUT_SIZE")
    print("=" * 50)
    
    print("Este patch solucionarÃ¡:")
    print("   âŒ Error 'input_size' en preprocessing")
    print("   âŒ Configuraciones faltantes en CUDA processor")
    
    if apply_input_size_patch():
        print("\nâœ… Segundo patch aplicado exitosamente")
        
        create_complete_test()
        
        print("\nğŸ§ª EJECUTA EL TEST FINAL:")
        print("   python test_final_complete.py")
        
        print("\nğŸ’¡ Si aÃºn hay errores:")
        print("   1. Reinicia Python completamente")
        print("   2. Ejecuta el test final")
        print("   3. Revisa los logs detallados")
    else:
        print("\nâŒ Error aplicando segundo patch")

if __name__ == "__main__":
    main()